---
title: 'Mastering the Game of Go without Human Knowledge'
date: 2023-11-20
permalink: /posts/2012/08/blog-post-1/
tags:
  - Go
  - AlphaGo
  - Reinforcement Learning
  - Deep Learning
  - Machine Learning
  - Artificial Intelligence
  - Game Theory
  - Monte Carlo Tree Search
  - Neural Networks
  - Convolutional Neural Networks
---

<!-- 
# Todo
- Scale images
  - Humans
    - Lee Sedol
    - Ke Jie
  - Policy Value Network
-->

# Go is interesting
Go is a board game that originated in China more than 2,500 years ago. The game is played by two players who take turns placing black and white stones on a 19x19 grid. The goal of the game is to surround more territory than your opponent. The game is considered to be one of the most complex board games in the world. The number of possible board positions is estimated to be $10^{170}$. For comparison, the number of atoms in the observable universe is estimated to be $10^{80}$. Globally it is played by over 40 million people.

# We can represent Go Positions Compactly
Input 17 x 19 x 19
One 19 x 19 feature map for white
One 19 x 19 feature map for black
Seven past board states for both colors
One 19 x 19 feature map indicating whos move it is


# Input and output representation
The input to the model is a 19x19x17 matrix. The first 17 channels represent the board state. The last channel represents the current player. The output of the model is a 19x19 matrix. Each element in the matrix represents the probability of the current player playing a stone at that position. The model is trained using a combination of supervised learning and reinforcement learning.

# Deep Learning Architecture


# Basic Cnn Example

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/CNN.jpg" alt="Image of CNN"/>
  <figcaption>Source: https://de.mathworks.com/discovery/convolutional-neural-network-matlab.html</figcaption>
</figure>

# Mapping Go to a Deep Learning Problem

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Policy_Value_Network.png" alt="Image of Policy and Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# How AlphaGo was trained

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaGo_TrainingAndEvaluation.png" alt="Image of Training and Evaluation"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Exhaustive Search is not feasible

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ExhaustiveSearch.png" alt="Image of Exhaustive Search Tree"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Use the Policy Network to reduce the breadth

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ReducingBreadthPolicyNetwork.png" alt="Image of reducing breadth with  Policy Network" width="10"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Use the Value Network to reduce the depth

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ReducingDepthValueNetwork.png" alt="Image of reducing depth with Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo uses Monte Carlo Tree Search

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/MCTS.png" alt="Image of MCTS"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo achieved superhuman performance

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/LeeSedol.png" alt="Image of Lee Sedol"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/KeJie_Half.png" alt="Image of Ke Jie"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

- AlhpaGo Lee in March 2016
- AlphaGo Master in May 2017
- AlphaGo Zero in October 2017

# AlphaGo Zero has Differences

- Only self-play
- Residual Network
- Single Neural Network
- Simplifyed MCTS

# AlhaGo Zero uses ResNets

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ResNet.png" alt="Image of ResNet"/>
  <figcaption>Source: He, Kaiming et al., 2015</figcaption>
</figure>

# AlphaGo Zero plays against itself

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_RL.png" alt="Image of RL in AlphaGo Zero"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# The Policy Network predicts AlphaGo Zeros move

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_PolicyNetwork.png" alt="Image of Policy Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# The Value Network predicts the winner

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_ValueNetwork.png" alt="Image of Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Results


# AlphaGo Zero has improved

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_Performance.png" alt="Image of Performance"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo becomes more efficient

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Power_Consumption.png" alt="Image of Power Consumption"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo Zero discovers opening patterns

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_Pattern.png" alt="Image of AlphaZero Patterns"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Humans can learn from AlphaGo Zero

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Joseki_Frequency.png" alt="Image of Joseki Frequency"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Disscussion

- Pros
  - Superior Performance
  - Self-Learning
  - Generalization

- Cons
  - Computational Resources
  - Lack of Explainability

# Conclusion

AlphaGo is the first computer program to defeat a world champion in Go.
AlphaGo Zero is able to learn Go from scratch.
Simpler algorithms can lead to better results.

# Acknowledgements

I would like to thank Prof. Dr. Mathias Niepert and M.Sc. Marimuthu Kalimuthu, for their guidance and support throughout this project. I would also like to thank my family for their support and encouragement.

# References

- https://www.youtube.com/watch?v=gsbkPpoxGQk
- https://www.youtube.com/watch?v=0slFo1rV0EM
- https://www.youtube.com/watch?v=Wujy7OzvdJk
- https://www.youtube.com/watch?v=Z1BELqFQZVM
- https://de.mathworks.com/discovery/convolutional-neural-network-matlab.html
- https://arxiv.org/pdf/1512.03385v1.pdf
- https://deepmind.google/technologies/alphago/
- https://deepmind.google/discover/blog/alphago-zero-starting-from-scratch/
- https://deepmind.google/discover/blog/innovations-of-alphago/

Silver, David, et al. 
"Mastering the game of Go with deep neural networks and tree search." nature 529.7587 (2016): 484-489.
"Mastering the game of go without human knowledge." nature 550.7676 (2017): 354-359.
"Mastering chess and shogi by self-play with a general reinforcement learning algorithm." arXiv preprint arXiv:1712.01815 (2017).
Blog Posts (Last accessed at 21.12.2023)
AlphaGo - Google DeepMind
AlphaGo Zero: Starting from scratch - Google DeepMind
Innovations of AlphaGo - Google DeepMind

# ChatGPT

# How AlphaGo and AlphaZero revolutionized the game of Go and beyond

Go is an ancient board game that originated in China more than 3000 years ago. It is played by over 40 million people worldwide, and has been considered a pinnacle of human intelligence and creativity. The game is simple to learn, but extremely complex to master. The number of possible positions on the 19x19 board is estimated to be around $10^{170}$, which is more than the number of atoms in the observable universe. This makes Go a formidable challenge for artificial intelligence, as traditional search methods are not suitable for exploring such a vast search space.

## The rise of AlphaGo

In 2016, a team of researchers from Google DeepMind announced a breakthrough in the field of computer Go. They developed a program called AlphaGo, which was able to defeat Lee Sedol, one of the greatest Go players of the last decades and the winner of 18 world titles, by a score of 4-1 in a historic match in Seoul. AlphaGo was also able to beat Ke Jie, the world's number one ranked player at the time, by a score of 3-0 in a match in China in 2017. AlphaGo also won 60 out of 60 games against other top professional players online.

How did AlphaGo achieve such a remarkable feat? The heart of AlphaGo was a combination of two convolutional neural networks (CNNs) that represented knowledge about the game of Go. The first network was called the policy network, which represented the recommendation of which move to play in a given position. The input to the policy network was the game board, which was fed through many convolutional layers and abstracted to features. The output was a policy distribution, which suggested which move to play.

The second network was called the value network, which represented the positional evaluation knowledge. The value network predicted the probability of winning the game from a given position, based on the board state and the player to move.

The training of AlphaGo involved three stages: supervised learning, reinforcement learning, and self-play. In the supervised learning stage, the policy network was trained on a large dataset of human expert moves, to mimic the human style of play. In the reinforcement learning stage, the policy network was further improved by playing against itself, and learning from its own successes and failures. The value network was also trained by using the outcomes of the self-play games as labels.

The final stage was self-play, where AlphaGo played millions of games against itself, using a technique called Monte Carlo tree search (MCTS) to guide its decisions. MCTS is a method that balances exploration and exploitation, by building a search tree that grows with each iteration. At each node of the tree, AlphaGo used its policy and value networks to evaluate the position and select the best move. The search tree was then expanded and updated with the results of the simulation.

AlphaGo used its neural networks to reduce the search tree in two ways: by reducing the breadth, and by reducing the depth. By reducing the breadth, AlphaGo only looked at the most promising moves suggested by the policy network, instead of considering all possible moves. By reducing the depth, AlphaGo estimated the value of the remaining moves by using the value network, instead of simulating the game until the end.

By using this approach, AlphaGo was able to focus on the most important parts of the tree, and search them systematically deeper. This gave AlphaGo an edge over its human opponents, who often suffered from cognitive biases and errors, such as delusion (mis-evaluating the position for 20-30 moves) or overconfidence (underestimating the opponent's strength).

## The evolution of AlphaGo Zero and AlphaZero

In 2017, the DeepMind team announced another breakthrough: AlphaGo Zero. AlphaGo Zero was a version of AlphaGo that learned to play Go from scratch, without any human knowledge or data, except for the rules of the game. AlphaGo Zero achieved this by using a single neural network, instead of two, that combined both the policy and the value functions. This network was trained solely by self-play reinforcement learning, starting from random play. AlphaGo Zero also used a simpler search algorithm, that did not rely on any randomized rollouts, but only used the neural network to evaluate the positions.

AlphaGo Zero was able to surpass the performance of AlphaGo in a matter of days, by becoming its own teacher. From each position, AlphaGo Zero executed MCTS using its neural network, and then updated the network with the results of the search. The network then predicted what action was chosen by the entire MCTS, which was a form of compression. This process was repeated over and over, generating better players and better data.

AlphaGo Zero was similar to a method called policy iteration, which is a technique for finding optimal policies in Markov decision processes. However, instead of using a greedy policy improvement, AlphaGo Zero used MCTS to select actions that were better than those suggested by the raw network. This way, AlphaGo Zero was not evaluating the raw network, but an improved lookahead version of it.

This was a form of search-based policy improvement, which was then followed by a search-based policy evaluation. AlphaGo Zero played over 30 million games against itself, and improved its network with each iteration. After three days, AlphaGo Zero defeated AlphaGo Lee, the version that beat Lee Sedol, by a score of 100-0. After 21 days, AlphaGo Zero defeated AlphaGo Master, the version that beat Ke Jie, by a score of 89-11.

One of the most remarkable aspects of AlphaGo Zero was that it rediscovered human opening patterns that took thousands of years to develop, as well as some novel moves that were discarded or overlooked by humans. AlphaGo Zero also demonstrated a more creative and dynamic style of play, that was less influenced by human biases and conventions.

The DeepMind team did not stop there. They generalized the approach of AlphaGo Zero to other board games, such as chess and shogi (Japanese chess). They developed a program called AlphaZero, which used the same algorithm and network architecture as AlphaGo Zero, but with different input and output layers to accommodate the different rules and board sizes. AlphaZero was able to learn chess and shogi from scratch, and defeat the world's best programs in both games, such as Stockfish (the 2016 world chess champion) and Elmo (the 2017 world shogi champion).

AlphaZero also showed a remarkable level of understanding and intuition in both games, by playing moves that were unconventional, but effective. AlphaZero also demonstrated a preference for dynamic and aggressive play, rather than passive and defensive play, which was typical of previous computer programs.

AlphaZero was a testament to the power and generality of reinforcement learning, combined with deep neural networks and Monte Carlo tree search. AlphaZero was able to master three different games, with different levels of complexity and action spaces, by using the same algorithm and network architecture, and without any human knowledge or data.

## The implications of AlphaGo and AlphaZero

The achievements of AlphaGo and AlphaZero have profound implications for the fields of artificial intelligence and computer science, as well as for the games of Go, chess, and shogi. AlphaGo and AlphaZero have shown that it is possible to create artificial agents that can surpass human performance in complex and strategic domains, by using self-learning and self-improvement techniques. AlphaGo and AlphaZero have also shown that it is possible to create artificial agents that can discover new knowledge and insights, by exploring the vast and rich spaces of these games.

AlphaGo and AlphaZero have also challenged the human players and fans of these games, by introducing new perspectives and possibilities. AlphaGo and AlphaZero have inspired human players to learn from their moves, and to improve their own skills and understanding. AlphaGo and AlphaZero have also stimulated human curiosity and creativity, by revealing new and unexpected aspects of these games.

AlphaGo and AlphaZero are not the end of the journey, but the beginning. There are still many open questions and challenges, such as how to transfer the learned knowledge and skills to other domains, how to explain the reasoning and decisions of the artificial agents, and how to ensure the ethical and social implications of their actions. AlphaGo and AlphaZero are also not the enemies, but the allies. They are not here to replace humans, but to augment them. They are not here to diminish the beauty and value of these games, but to enhance them. They are not here to end the human quest for intelligence and creativity, but to join it.
