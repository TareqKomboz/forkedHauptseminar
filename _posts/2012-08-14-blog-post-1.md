---
title: 'Mastering the Game of Go without Human Knowledge'
date: 2023-11-20
permalink: /posts/2012/08/blog-post-1/
tags:
  - Go
  - AlphaGo
  - Reinforcement Learning
  - Deep Learning
  - Machine Learning
  - Artificial Intelligence
  - Game Theory
  - Monte Carlo Tree Search
  - Neural Networks
  - Convolutional Neural Networks
---

<!-- 
Todo
- Exact architecture of the networks (number of layers, number of filters, etc.)
- Which RL algorithm was used for the self-play stage?
- Writing
  - Google
  - DeepMind
  - AlphaGo
  - AlphaGo Zero
  - AlphaZero
  - value network
  - policy network
  - Monte Carlo Tree Search
- check alternative descriptions
- Add
  - 40 residual blocks
-->

# Go is interesting
Go is a board game that originated in China more than 2,500 years ago. The game is played by two players who take turns placing black and white stones on a 19x19 grid. The goal of the game is to surround more territory than your opponent. The game is considered to be one of the most complex board games in the world. The number of possible board positions is estimated to be $10^{170}$. For comparison, the number of atoms in the observable universe is estimated to be $10^{80}$. Globally it is played by over 40 million people.

# A 1-vs-1 board game

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Go.png" alt="Image of Go Board"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

It is played by two players, Black and White, who take turns placing stones of their color on the intersections of a 19x19 grid. The objective of the game is to surround more territory than the opponent, as well as capture the opponent’s stones by depriving them of liberties. A liberty is an adjacent empty point on the board. A stone or a group of connected stones of the same color is captured and removed from the board when it has no more liberties. A player can pass instead of placing a stone, and the game ends when both players pass consecutively. The score is then calculated by counting the number of empty points enclosed by each player’s stones, plus the number of captured stones. The player with the higher score wins the game. 

Go is characterized by perfect information, meaning both players have complete knowledge of the entire state of the game at all times. Every move made is visible and known to both Black and White, ensuring that there are no secrets or hidden information. This aspect ensures a level of strategic depth and complexity as players can plan their moves based on complete information.

Furthermore, Go is a zero-sum game. In this context, it means that one player’s gain is equivalent to another’s loss. The total utility to be distributed in the game remains constant; thus, the competition is strict and direct. Every point scored or stone captured by one player signifies a loss for the opponent.

Lastly, there’s no draw in Go due to specific rules like superko or positional superko that prevent infinite repetitions and ensure a decisive outcome. Each game concludes with a clear winner and loser as both players strive not only for territorial control but also aim to capture their opponent’s stones effectively. The intricate balance between offense and defense, coupled with the impossibility of a draw, makes every move critical in determining the ultimate victor.

There are some additional rules to prevent infinite repetitions and ensure a fair result, such as the ko rule and the seki rule. For more details on the rules of go, you can refer to this [Wikipedia article](https://en.wikipedia.org/wiki/Go_(game)).

# Represent Go positions compactly

To train and evaluate AlphaGo, the algorithm needs to represent the state of the go board in a way that is suitable for neural networks. The representation used by AlphaGo is based on a set of 17 feature maps, each of which is a 19 x 19 matrix of binary values. The first two feature maps indicate the presence of white and black stones on the board, respectively. The next 14 feature maps encode the history of the board by showing the positions of white and black stones for the last seven moves. This allows the algorithm to capture the temporal dynamics of the game and the effects of the ko rule and acts as a sort of attention mechanism to the last moves. The last feature map indicates whose turn it is to play by having a value of 1 for all points if it is black’s turn, or 0 otherwise. The 17 feature maps are then stacked together to form a 17 x 19 x 19 input tensor for the neural networks. This representation is simple, yet effective, as it captures the essential information about the board state and the game rules.

# Mapping Go to a Deep Learning Problem

The heart of the AlphaGo alogorithm is a combination of two neural networks, a policy network and a value network. The policy network is a deep convolutional neural network (CNN) that takes as input the current board position and outputs a probability distribution over all possible moves. The value network is also a deep CNN that takes as input the current board position and outputs a real-valued number between -1 and 1, which represents the predicted probability of winning the game from the current position. The policy network is used to guide the search for the best move, while the value network is used to evaluate the position and guide the search for the best move.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/both_networks.PNG" alt="Image of Policy and Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Training procedure

The training of AlphaGo involved two stages: supervised learning and self-play via Reinforcement Learning (RL). 

In the supervised learning stage, the policy network was trained on a large dataset of human expert moves, consisiting of 30 million positions, to mimic the human style of play. That means for a given board position, the policy network outputs a probability distribution over all possible moves, and the move that was actually played by the human expert is used as the target label. By calculating the cross-entropy loss between the predicted and the actual move, the policy network becomes better at predicting human moves. 
The outcome of this is a policy given by the Policy Network that can predict human moves with high accuracy and is therfore comparably good as human experts.

In the self-play stage, where AlphaGo played millions of games against itself, this policy was further improved by RL, promoting moves that lead to successes and surpressing moves that lead to failures. 
Interestingly, the self-play was between the current best policy and a policy from a random previous iteration to prevent overfitting. 

After, the value network was trained by using the outcomes of the self-play games as labels. Using the Mean Squared Error loss function, the value network becomes better at predicting the probability of winning the game from a given position.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaGo_TrainingAndEvaluation.png" alt="Image of Training and Evaluation"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

In the following we are going to take a closer look at why using these two NNs is necessary and beneficial.

# Exhaustive search is not feasible

In theory, the perfect game of Go can be played by calculating all possible continuations from a certain postion and choosing the move that leads statistically to the highest number of wins. However, the number of possible moves per position is 150 on average, and a typical game of Go lasts for 250 moves. This makes it infeasible to use an exhaustive search to find the best move, as it would require evaluating an astronomical number of positions. Even with the fastest computers available, it would take longer than the age of the universe to evaluate all possible positions. Therefore, a more efficient approach is needed to find the best move in a reasonable amount of time. 

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ExhaustiveSearch.png" alt="Image of Exhaustive Search Tree"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Use the Policy Network to reduce the breadth

The Policy Network is used to reduce the breadth of the search tree by focusing on the most promising moves. The Policy Network predicts the probability distribution over all possible moves for a given board position. The search tree then only considers the moves with the highest probability, instead of evaluating all possible moves. This reduces the number of nodes in the search tree and makes the search more efficient.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ReducingBreadthPolicyNetwork.png" alt="Image of reducing breadth with  Policy Network" width="10"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Use the Value Network to reduce the depth

To know whether a move is good or not one could think one would have to play the game until the end and see who wins. However, If one would have a perfect Value Network that could predict the probability of winning with very high certainty, than one would have to only simulate one move deep into the future and pick the one for which the Value Network predicts the highest probability of winning. This would reduce the depth of the search tree and make the search more efficient. 
The Value Network is used to reduce the depth of the search tree by estimating the value of the remaining moves without simulating the game until the end. This reduces the number of nodes in the search tree and makes the search more efficient.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ReducingDepthValueNetwork.png" alt="Image of reducing depth with Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo uses Monte Carlo Tree Search

Monte Carlo Tree Search (MCTS) is a method that balances exploration and exploitation, by building a search tree that grows with each iteration.

It is composed of the following four steps:
1. Selection: Starting from the root node, the tree is traversed by iterativly selecting the most promising child node based on choosing the action that maximizes the metric $Q(s, a) + u(s, a) = \frac{W(s, a)}{N(s, a)} + c_{puct}P(s, a)\frac{\sqrt{\sum_{b} N(s, b)}}{1 + N(s, a)}$ until a leaf node is reached.
2. Expansion: The selected leaf node is expanded by adding the most probable child nodes to the tree, according to the probability outputed by the Policy Network.
3. Evaluation: All expanded nodes are evaluated by performing simulations of how the game might continue until the end of the game is reached. For the simulation, moves are sampled according to the probability distribution provided by the policy network. The result of the simulation is then used combination with the estimation provided by the value network to update the search tree.
4. Backpropagation: The result of the evaluation is backpropagated up the search tree to update the statistics of the nodes that were visited during the selection phase. After that the next iteration starts.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/MCTS.png" alt="Image of MCTS"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo achieved superhuman performance

In March 2016, the inevitable happened: AlphaGo was able to defeat Lee Sedol, one of the greatest Go players of the last decades and the winner of 18 world titles, by a score of 4-1 in a historic match in Seoul. This was the first time that a computer program was able to defeat a world champion in Go. The match was watched by millions of people around the world and was considered a milestone in the development of artificial intelligence.

In May 2017, AlphaGo Master, an successor AI developed by DeepMind, faced off against Ke Jie, who was then ranked as the world's number one player. The match took place in China and ended with AlphaGo Master emerging victorious with a score of 3-0.
But this wasn't AlphaGo Master's only achievement. It also played online against various Go masters and astonishingly won 60 out of 60 games. Further, it did not loose once to its predecessor version. This impressive performance can be attributed to the use of state-of-the-art deep residual networks (ResNets) instead of CNNs and even more iterations of RL during training.

One could think that there is not much more to achieve after such a performance. However, the DeepMind team did not stop there. In October 2017, they developed a program called AlphaGo Zero.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Timeline.PNG" alt="Image of Timeline"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>
___

# The main differences of AlphaGo Zero
AlphaGo Zero was yet another significant breakthrough, as it demonstrated the power of self-learning and self-improvement in artificial intelligence. The main differences of AlphaGo Zero compared to AlphaGo are:
- Only self-play: AlphaGo Zero was trained solely by self-play reinforcement learning, starting from random play, without any human data or knowledge of the game. This allowed AlphaGo Zero to discover new strategies and patterns that were not known to human players.
- Single Neural Network: Instead of two separate networks for the policy and value functions, AlphaGo Zero used a single neural network that combined both functions.
- Simpler tree search: AlphaGo Zero used a simpler and faster tree search algorithm that did not rely on any randomized rollouts in the evaluation step, but only used the value network to evaluate the positions.

# AlphaGo Zero plays against itself

Initially, both the single network is initialized with random weights. AlphaGo Zero chooses moves using MCTS and the current network to sample moves for both colors. We end up with one game of Go, with which we can update the network. The probability distribution of moves can be compared to what actually happend and the networks value estimation is updated by using the outcome of the game to train the network. By playing this one game, the accuracy of the network improved slightly. Now this improved network is used to play another game of Go, which probably is of higher quality than the first one, due to the improved network. This process is repeated over and over, generating better players and better data.

 
  - 29 million games of self-play

Then, AlphaGo Zero plays games against itself, using the current best policy and value network to guide the search for the best move. After each game, the neural network is updated with the results of the search. This process is repeated over and over, generating better players and better data.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_RL.png" alt="Image of RL in AlphaGo Zero"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# The Policy Network predicts AlphaGo Zero's move

Previously, the policy network was trained on a large dataset of human expert moves. 

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_PolicyNetwork.png" alt="Image of Policy Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# The Value Network predicts the winner

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_ValueNetwork.png" alt="Image of Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# The loss function consists of Mean Squared Error and Cross-Entropy Loss

The most interesting part of combining the policy and the value network into one is to observe how the loss function changes. The loss function is a combination of the two well-known Mean Squared Error loss function and the Cross-Entropy loss function with L2-Regularization. The Mean Squared Error part is needed to train the value network, while the Cross-Entropy loss part is needed to train the policy network.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Loss_annotated.PNG" alt="Image of Loss"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo Zero has improved

At day zero, AlphaGo Zero starts by playing random moves. After three days of training, AlphaGo Zero defeated AlphaGo Lee, the version that beat Lee Sedol, by a score of 100-0. After 27 days, AlphaGo Zero defeated AlphaGo Master, the version that beat Ke Jie, by a score of 89-11. After 40 days, AlphaGo Zero became the best Go player in the world, surpassing all previous versions of AlphaGo.

On the right one can see 

Grey Raw Network
Motivate Patterns

The evaluation of the strength of the algorithms was done by playing games against previous versions of AlphaGo. The algorithms had five seconds of thinking time per move and 1600 simulations for each MCTS. The results are shown in the following figure.


<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaGoZero_Performance.png" alt="Image of AlphaGoZero_Performance"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo Zero discovers opening patterns

Over time humans came up with patterns of moves (especially in the beginning of a go match) that are considered to be the best. These patterns are called josekis. One of the most remarkable aspects of AlphaGo Zero is that it rediscovered human opening patterns that took thousands of years to develop, as well as some novel moves that were discarded or overlooked by humans.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_Pattern.png" alt="Image of AlphaZero Patterns"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Humans can learn from AlphaGo Zero

AlphaGo Zero not only rediscovered know josekis, but also discovered new ones. This shows that the algorithm is able to learn from scratch and discover new strategies and patterns that were not known to human players. This is a remarkable demonstration of the power of self-learning and self-improvement in artificial intelligence.
Further, some josekis that were considered to be the best by humans were discarded by AlphaGo Zero, showing that the algorithm is able to challenge and improve human knowledge.
The following figure shows that AlphaGo Zero picks up some patterns but discardes them after more training if deemed inferior.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Joseki_Frequency.png" alt="Image of Joseki Frequency"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Discussion

## Contributions

- **Superior Performance**: AlphaGo and its successor, AlphaGo Zero, have demonstrated superior performance of algorihtms over humans in the game of Go for the first time in history. Their victories against world-class human players have underscored the potential of artificial intelligence in mastering complex tasks.

- **Self-Learning**: One of the most remarkable aspects of AlphaGo Zero is its ability to learn from scratch, without any prior human knowledge. Through a process of self-play, it was able to acquire strategies and tactics that even the most skilled human players had not discovered.

- **Generalization**: The techniques used in AlphaGo are not limited to the game of Go. They represent a general approach to artificial intelligence that can be applied to a wide range of problems. Actually, the Google DeepMind Team proved that the same concept achieve in many other games like chess and shogi. This opens up exciting possibilities for future research and applications.

## Limitations

- **Lack of Explainability**: While the performance of AlphaGo and AlphaGo Zero is impressive, the neural networks they use are complex and difficult to interpret. This lack of explainability makes it challenging to understand the reasoning behind the moves made by the algorithms. Especially Beginners in Go might not find value in move suggestions as a move is only strong when understanding the follow up plan.

- **Data Efficiency**: AlphaGo Zero required a large amount of data—equivalent to more than a single human's lifetime experience—to achieve its performance. While this is still more efficient than previous methods, this suggests that there may still be room for improving the data efficiency of AI systems. On the other hand, it's worth noting that human players achieve high levels of skill by learning from the accumulated knowledge of previous generations through books and videos.
  
# Conclusion

In conclusion, the development and success of AlphaGo have marked a significant milestone in the field of artificial intelligence. AlphaGo Lee, by integrating existing Monte Carlo Tree Search (MCTS) techniques with Deep Learning, became the first computer program to triumph over a world champion in the complex game of Go. This achievement underscored the potential of combining traditional AI methods with modern machine learning techniques.

However, the evolution didn’t stop there. AlphaGo Zero, a subsequent version, demonstrated an even more profound capability. Trained entirely from scratch, without relying on any pre-existing human game data, it managed to outperform all other algorithms in Go. This was achieved through a process of self-learning, highlighting the power of autonomous AI systems.

Interestingly, the journey of AlphaGo also revealed that simpler tree search algorithms and architectures can sometimes lead to superior results. This insight challenges the common notion that complexity is always synonymous with effectiveness, and opens up new avenues for exploration in the realm of artificial intelligence.

In essence, the story of AlphaGo is a testament to the remarkable strides being made in AI, and a glimpse into the exciting possibilities that lie ahead. 

# Acknowledgements

I would like to thank Prof. Dr. Mathias Niepert and M.Sc. Marimuthu Kalimuthu, for their guidance and support throughout this project. 

# References

- https://www.youtube.com/watch?v=gsbkPpoxGQk
- https://www.youtube.com/watch?v=0slFo1rV0EM
- https://www.youtube.com/watch?v=Wujy7OzvdJk
- https://www.youtube.com/watch?v=Z1BELqFQZVM
- https://de.mathworks.com/discovery/convolutional-neural-network-matlab.html
- https://arxiv.org/pdf/1512.03385v1.pdf
- https://deepmind.google/technologies/alphago/
- https://deepmind.google/discover/blog/alphago-zero-starting-from-scratch/
- https://deepmind.google/discover/blog/innovations-of-alphago/

Silver, David, et al. 
"Mastering the game of Go with deep neural networks and tree search." nature 529.7587 (2016): 484-489.
"Mastering the game of go without human knowledge." nature 550.7676 (2017): 354-359.
"Mastering chess and shogi by self-play with a general reinforcement learning algorithm." arXiv preprint arXiv:1712.01815 (2017).
Blog Posts (Last accessed at 21.12.2023)
AlphaGo - Google DeepMind
AlphaGo Zero: Starting from scratch - Google DeepMind
Innovations of AlphaGo - Google DeepMind

# ChatGPT

# How AlphaGo and AlphaZero revolutionized the game of Go and beyond

Go is an ancient board game that originated in China more than 3000 years ago. It is played by over 40 million people worldwide, and has been considered a pinnacle of human intelligence and creativity. The game is simple to learn, but extremely complex to master. The number of possible positions on the 19x19 board is estimated to be around $10^{170}$, which is more than the number of atoms in the observable universe. This makes Go a formidable challenge for artificial intelligence, as traditional search methods are not suitable for exploring such a vast search space.

## The rise of AlphaGo

In 2016, a team of researchers from Google DeepMind announced a breakthrough in the field of computer Go. They developed a program called AlphaGo, which was able to defeat Lee Sedol, one of the greatest Go players of the last decades and the winner of 18 world titles, by a score of 4-1 in a historic match in Seoul. AlphaGo was also able to beat Ke Jie, the world's number one ranked player at the time, by a score of 3-0 in a match in China in 2017. AlphaGo also won 60 out of 60 games against other top professional players online.

How did AlphaGo achieve such a remarkable feat? The heart of AlphaGo was a combination of two convolutional neural networks (CNNs) that represented knowledge about the game of Go. The first network was called the policy network, which represented the recommendation of which move to play in a given position. The input to the policy network was the game board, which was fed through many convolutional layers and abstracted to features. The output was a policy distribution, which suggested which move to play.

The second network was called the value network, which represented the positional evaluation knowledge. The value network predicted the probability of winning the game from a given position, based on the board state and the player to move.

The training of AlphaGo involved three stages: supervised learning, reinforcement learning (RL), and self-play. In the supervised learning stage, the policy network was trained on a large dataset of human expert moves, to mimic the human style of play. In the RL stage, the policy network was further improved by playing against itself, and learning from its own successes and failures. The value network was also trained by using the outcomes of the self-play games as labels.

The final stage was self-play, where AlphaGo played millions of games against itself, using a technique called Monte Carlo tree search (MCTS) to guide its decisions. MCTS is a method that balances exploration and exploitation, by building a search tree that grows with each iteration. At each node of the tree, AlphaGo used its policy and value networks to evaluate the position and select the best move. The search tree was then expanded and updated with the results of the simulation.

AlphaGo used its neural networks to reduce the search tree in two ways: by reducing the breadth, and by reducing the depth. By reducing the breadth, AlphaGo only looked at the most promising moves suggested by the policy network, instead of considering all possible moves. By reducing the depth, AlphaGo estimated the value of the remaining moves by using the value network, instead of simulating the game until the end.

By using this approach, AlphaGo was able to focus on the most important parts of the tree, and search them systematically deeper. This gave AlphaGo an edge over its human opponents, who often suffered from cognitive biases and errors, such as delusion (mis-evaluating the position for 20-30 moves) or overconfidence (underestimating the opponent's strength).

## The evolution of AlphaGo Zero and AlphaZero

In 2017, the DeepMind team announced another breakthrough: AlphaGo Zero. AlphaGo Zero was a version of AlphaGo that learned to play Go from scratch, without any human knowledge or data, except for the rules of the game. AlphaGo Zero achieved this by using a single neural network, instead of two, that combined both the policy and the value functions. This network was trained solely by self-play RL, starting from random play. AlphaGo Zero also used a simpler search algorithm, that did not rely on any randomized rollouts, but only used the neural network to evaluate the positions.

AlphaGo Zero was able to surpass the performance of AlphaGo in a matter of days, by becoming its own teacher. From each position, AlphaGo Zero executed MCTS using its neural network, and then updated the network with the results of the search. The network then predicted what action was chosen by the entire MCTS, which was a form of compression. This process was repeated over and over, generating better players and better data.

AlphaGo Zero was similar to a method called policy iteration, which is a technique for finding optimal policies in Markov decision processes. However, instead of using a greedy policy improvement, AlphaGo Zero used MCTS to select actions that were better than those suggested by the raw network. This way, AlphaGo Zero was not evaluating the raw network, but an improved lookahead version of it.

This was a form of search-based policy improvement, which was then followed by a search-based policy evaluation. AlphaGo Zero played over 30 million games against itself, and improved its network with each iteration. After three days, AlphaGo Zero defeated AlphaGo Lee, the version that beat Lee Sedol, by a score of 100-0. After 21 days, AlphaGo Zero defeated AlphaGo Master, the version that beat Ke Jie, by a score of 89-11.

One of the most remarkable aspects of AlphaGo Zero was that it rediscovered human opening patterns that took thousands of years to develop, as well as some novel moves that were discarded or overlooked by humans. AlphaGo Zero also demonstrated a more creative and dynamic style of play, that was less influenced by human biases and conventions.

The DeepMind team did not stop there. They generalized the approach of AlphaGo Zero to other board games, such as chess and shogi (Japanese chess). They developed a program called AlphaZero, which used the same algorithm and network architecture as AlphaGo Zero, but with different input and output layers to accommodate the different rules and board sizes. AlphaZero was able to learn chess and shogi from scratch, and defeat the world's best programs in both games, such as Stockfish (the 2016 world chess champion) and Elmo (the 2017 world shogi champion).

AlphaZero also showed a remarkable level of understanding and intuition in both games, by playing moves that were unconventional, but effective. AlphaZero also demonstrated a preference for dynamic and aggressive play, rather than passive and defensive play, which was typical of previous computer programs.

AlphaZero was a testament to the power and generality of RL, combined with deep neural networks and Monte Carlo tree search. AlphaZero was able to master three different games, with different levels of complexity and action spaces, by using the same algorithm and network architecture, and without any human knowledge or data.

## The implications of AlphaGo and AlphaZero

The achievements of AlphaGo and AlphaZero have profound implications for the fields of artificial intelligence and computer science, as well as for the games of Go, chess, and shogi. AlphaGo and AlphaZero have shown that it is possible to create artificial agents that can surpass human performance in complex and strategic domains, by using self-learning and self-improvement techniques. AlphaGo and AlphaZero have also shown that it is possible to create artificial agents that can discover new knowledge and insights, by exploring the vast and rich spaces of these games.

AlphaGo and AlphaZero have also challenged the human players and fans of these games, by introducing new perspectives and possibilities. AlphaGo and AlphaZero have inspired human players to learn from their moves, and to improve their own skills and understanding. AlphaGo and AlphaZero have also stimulated human curiosity and creativity, by revealing new and unexpected aspects of these games.

AlphaGo and AlphaZero are not the end of the journey, but the beginning. There are still many open questions and challenges, such as how to transfer the learned knowledge and skills to other domains, how to explain the reasoning and decisions of the artificial agents, and how to ensure the ethical and social implications of their actions. AlphaGo and AlphaZero are also not the enemies, but the allies. They are not here to replace humans, but to augment them. They are not here to diminish the beauty and value of these games, but to enhance them. They are not here to end the human quest for intelligence and creativity, but to join it.
