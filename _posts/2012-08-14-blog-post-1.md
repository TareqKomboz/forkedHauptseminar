---
title: 'Mastering the Game of Go'
date: 2023-11-20
permalink: /posts/2012/08/blog-post-1/
tags:
  - Go
  - AlphaGo
  - Reinforcement Learning
  - Deep Learning
  - Machine Learning
  - Artificial Intelligence
  - Game Theory
  - Monte Carlo Tree Search
  - Neural Networks
  - Convolutional Neural Networks
---

# Introduction?

<hr style="border: 1px solid black;">

Hello everyone! 
In this blog post, I will summarize, highlight and explain the most important concepts and ideas of the papers ["Mastering the game of Go with deep neural networks and tree search" by Silver, David et al. (2016)](https://www.nature.com/articles/nature16961) and ["Mastering the Game of Go without Human Knowledge" by Silver, David et al. (2017)](https://www.nature.com/articles/nature24270). 
The work of David Silver et al. marks a milestone in the field of artificial intelligence, as it presents the development of AlphaGo and AlphaGo Zero, two computer programs that have revolutionized the game of Go and demonstrated the power of deep learning and reinforcement learning in mastering complex tasks. 
I hope you enjoy reading this blog post, and find it informative and inspiring!

# Why Go?

<hr style="border: 1px solid black;">
 
Go is an ancient board game that originated in China more than 3000 years ago. 
It is played by over 40 million people worldwide, and has been considered a pinnacle of human intelligence and creativity. 
The game is simple to learn, but extremely complex to master. 
The number of possible positions on the 19x19 board is estimated to be around $10^{170}$, which is more than the number of atoms in the observable universe. 
Considering Deep Blue's victory over Garry Kasparov in 1997 in chess, Go was considered to be the next big challenge for artificial intelligence, as traditional search methods are not suitable for exploring such a vast search space.

# A 1-vs-1 Game on a 19x19 Board

<hr style="border: 1px solid black;">

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Go.png" alt="Image of Go Board"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

Feel free to skip this section if you are already familiar with the game of Go. 
For the rest of you, here is a brief introduction: 

The game of Go is played by two players, Black and White, who take turns placing stones of their color on the intersections of a 19x19 grid. 
The objective of the game is to surround more territory than the opponent, as well as capture the opponent’s stones by depriving them of liberties. 
A liberty is an adjacent empty point on the board. 
A stone or a group of connected stones of the same color is captured and removed from the board when it has no more liberties. 
A player can pass instead of placing a stone, and the game ends when both players pass consecutively. 
The score is then calculated by counting the number of empty points enclosed by each player’s stones, plus the number of captured stones. The player with the higher score wins the game. 

Go is characterized by perfect information, meaning both players have complete knowledge of the entire state of the game at all times. Every move made is visible and known to both Black and White, ensuring that there are no secrets or hidden information. 
Furthermore, there’s no draw in Go due to specific rules like superko or positional superko that prevent infinite repetitions and ensure a decisive outcome. 
Each game concludes with a clear winner and loser.

There are some additional rules to prevent infinite repetitions and ensure a fair result, such as the ko rule and the seki rule. 
For more details on the rules of go, you can refer to this [Wikipedia article](https://en.wikipedia.org/wiki/Go_(game)).

# How to represent Go positions?

<hr style="border: 1px solid black;">

To train and evaluate AlphaGo, the algorithm needs to represent the state of the go board in a way that is suitable for neural networks. The representation used by AlphaGo is based on a set of 17 feature maps, each of which is a 19 x 19 matrix of binary values. 
The first two feature maps indicate the presence of white and black stones on the board, respectively. 
The next 14 feature maps encode the history of the board by showing the positions of white and black stones for the last seven moves. 
This allows the algorithm to capture the temporal dynamics of the game and the effects of the ko rule and acts as a sort of attention mechanism to the last moves.
The last feature map indicates whose turn it is to play by having a value of 1 for all points if it is black’s turn, or 0 otherwise. 
The 17 feature maps are then stacked together to form a 17 x 19 x 19 input tensor for the neural networks. 
This representation is simple, yet effective, as it captures the essential information about the board state and the game rules.

# Mapping Go to a Deep Learning Problem

<hr style="border: 1px solid black;">
 
The heart of the AlphaGo alogorithm is a combination of two neural networks, a policy network and a value network. 
Now that we know how to represent the state of the go board, we can use this representation to train and evaluate these neural networks. 
The policy network is a [deep convolutional neural network (CNN)](https://arxiv.org/abs/1511.08458) that takes as input the current board position and outputs a probability distribution over all possible moves, suggesting which move to play. 
The value network is also a deep CNN that takes as input the current board position and outputs a real-valued number between -1 and 1, which represents the predicted probability of winning the game from the current position. 
The policy network is used to guide the search for the best move, while the value network is used to evaluate the position and guide the search for the best move.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/both_networks.PNG" alt="Image of Policy and Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# The Training Procedure

<hr style="border: 1px solid black;">

The training of AlphaGo can be divided into three stages:
1. Training the policy network [(Supervised Learning)](https://en.wikipedia.org/wiki/Supervised_learning)
2. Improving the policy of the policy network [(Reinforcement Learning)](https://lilianweng.github.io/posts/2018-02-19-rl-overview/)
3. Training the value network (Supervised Learning)

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaGo_TrainingAndEvaluation.png" alt="Image of Training and Evaluation"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

## First Stage
In the first stage, the policy network was trained on a large dataset of human expert moves, consisiting of 30 million positions, to mimic the human style of play. 
That means for a given board position, the policy network outputs a probability distribution over all possible moves, and the move that was actually played by the human expert is used as the target label. 
By calculating the cross-entropy loss between the predicted and the actual move, the policy network becomes better at predicting human moves. 
The outcome of this is a policy given by the policy network that can predict human moves with high accuracy and is therfore comparably good as human experts.

## Second Stage
In the second stage, where AlphaGo played millions of games against itself, this policy was further improved by RL, promoting moves that lead to successes and surpressing moves that lead to failures. 
Interestingly, the self-play was between the current best policy and a policy from a random previous iteration to prevent overfitting. 

## Third Stage
After, the value network was trained by using the outcomes of the self-play games as labels. 
Using the Mean Squared Error loss function, the value network becomes better at predicting the probability of winning the game from a given position.

# Monte Carlo Tree Search (MCTS) in AlphaGo
<hr style="border: 1px solid black;">

Before I explain to you the actual MCTS, let us first understand why naive approaches fail and how using the two trained neural networks (NNs) is necessary and beneficial.

## Exhaustive Search is not feasible

In theory, the perfect game of Go can be played by calculating all possible continuations from a certain postion and choosing the move that leads statistically to the highest number of wins. 
However, the number of possible moves per position is 150 on average, and a typical game of Go lasts for 250 moves. 
This makes it infeasible to use an exhaustive search to find the best move, as it would require evaluating an astronomical number of positions. 
Even with the fastest computers available, it would take longer than the age of the universe to evaluate all possible positions. Therefore, a more efficient approach is needed to find the best move in a reasonable amount of time.
AlphaGo does this using its two neural networks to reduce the search tree in two ways: by reducing the breadth, and by reducing the depth. 

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ExhaustiveSearch.png" alt="Image of Exhaustive Search Tree"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

## Reducing the Breadth with the Policy Network

Remember, the policy network predicts the probability distribution over all possible moves for a given board position.
This policy network can be used to reduce the breadth of the search tree by focusing on the most promising moves. 
Instead of evaluating all possible moves, the search tree then only considers the moves with the highest probability. 
This reduces the number of nodes in the search tree and makes the search more efficient.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ReducingBreadthPolicyNetwork.png" alt="Image of reducing breadth with Policy Network" width="10"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

## Reducing the Depth with the Value Network

To know whether a move is good or not one could think one would have to play the game until the end and see who wins. 
However, if one would have a perfect estimator that could predict the probability of winning with perfect certainty, than one would have to only simulate one move deep into the future and pick the one for which the estimator predicts the highest probability of winning. 
We do not have a perfect estimator, but the value network beomes better and better with more training.
This value network can be used as an estimator to reduce the depth of the search tree by estimating the value of the remaining moves without simulating the game until the end. 
This reduces the number of nodes in the search tree and makes the search more efficient.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/ReducingDepthValueNetwork.png" alt="Image of reducing depth with Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

## Combination of MCTS with two Neural Networks

By using the two networks in combination with MCTS, AlphaGo was able to focus on the most important parts of the tree, and search them systematically deeper. This gave AlphaGo an edge over its human opponents, who often suffered from cognitive biases and errors, such as delusion (mis-evaluating the position for 20-30 moves) or overconfidence (underestimating the opponent's strength).

MCTS is a method that balances exploration and exploitation, by building a search tree that grows with each iteration.
It is composed of the following four steps:
1. Selection: Starting from the root node, the tree is traversed by iterativly selecting the most promising child node based on choosing the action that maximizes the metric $Q(s, a) + u(s, a) = \frac{W(s, a)}{N(s, a)} + c_{puct}P(s, a)\frac{\sqrt{\sum_{b} N(s, b)}}{1 + N(s, a)}$ until a leaf node is reached.
2. Expansion: The selected leaf node is expanded by adding the most probable child nodes to the tree, according to the probability outputed by the policy network.
3. Evaluation: All expanded nodes are evaluated by performing simulations of how the game might continue until the end of the game is reached. For the simulation, moves are sampled according to the probability distribution provided by the policy network. The result of the simulation is then used combination with the estimation provided by the value network to update the search tree.
4. Backpropagation: The result of the evaluation is backpropagated up the search tree to update the statistics of the nodes that were visited during the selection phase. After that the next iteration starts.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/MCTS.png" alt="Image of MCTS"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Performance of AlphaGo
<hr style="border: 1px solid black;">

In March 2016, the inevitable happened: AlphaGo was able to defeat Lee Sedol, one of the greatest Go players of the last decades and the winner of 18 world titles, by a score of 4-1 in a historic match in Seoul. 
This was the first time that a computer program was able to defeat a world champion in Go. 
The match was watched by millions of people around the world and was considered a milestone in the development of artificial intelligence.

In May 2017, AlphaGo Master, an successor AI developed by DeepMind, faced off against Ke Jie, who was then ranked as the world's number one player. 
The match took place in China and ended with AlphaGo Master emerging victorious with a score of 3-0.
But this wasn't AlphaGo Master's only achievement. 
It also played online against various Go masters and astonishingly won 60 out of 60 games. 
Further, it did not loose once to its predecessor version. 
This impressive performance can be attributed to the use of state-of-the-art deep [residual networks (ResNets)](https://arxiv.org/abs/1512.03385) instead of CNNs and even more iterations of RL during training.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Timeline.PNG" alt="Image of Timeline"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

One could think that there is not much more to achieve after such a performance, but the DeepMind team did not stop there. 
In October 2017, they published yet another succesor called AlphaGo Zero.

# AlphaGo Zero
<hr style="border: 2px solid black;">

In 2017, the DeepMind team announced another breakthrough: AlphaGo Zero. AlphaGo Zero was a version of AlphaGo that learned to play Go from scratch, without any human knowledge or data, except for the rules of the game. AlphaGo Zero achieved this by using a single neural network, instead of two, that combined both the policy and the value functions. This network was trained solely by self-play RL, starting from random play. AlphaGo Zero also used a simpler search algorithm, that did not rely on any randomized rollouts in the evaluation of a postion, but only used the neural network to evaluate the positions.

# AlphaGo Zero plays against itself
<hr style="border: 1px solid black;">

AlphaGo Zero was trained by playing 29 million games against itself, effectively becoming its own teacher.
Initially, the network in AlphaGo Zero is initialized with random weights. 
The system then uses MCTS and the current network to sample moves for both colors, resulting in completed games of Go.
These games serve as a basis for updating the network.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_RL.png" alt="Image of RL in AlphaGo Zero"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

The network still outputs a probability distribution over all possible moves for a given board position. 
However, instead of predicting the moves that human experts would make, the network is now trained to predict the moves that AlphaGo Zero itself made in the self-play games.
The probability distribution of moves generated by the network is compared to the actual moves made during the games. 
The network’s value estimation is updated based on the outcomes of these games. 
Consequently, the accuracy of the network can be improved through gameplay.

From each position, AlphaGo Zero executed MCTS using its neural network, and then updated the network with the results of the search. 
We want the raw neural network to predict the action that was chosen by an entire MCTS. 
One can think of trying to compress what was achieved using look ahead into the new neural networks.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_PolicyNetwork.png" alt="Image of AlphaGo Zero Policy Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_ValueNetwork.png" alt="Image of AlphaGo Zero Value Network"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

This improved network is then used to play more games. 
A more accurate network leads to better-guided MCTS, which in turn results in better moves. 
These high-quality moves can then be used to further improve the network. 
This cycle of improvement—better networks leading to better moves, which lead to better networks—is repeated continuously.
This process reminds of a method called policy iteration, a technique used for finding optimal policies in Markov decision processes.
However, instead of using a greedy policy improvement, AlphaGo Zero employs MCTS to select actions that outperform those suggested by the raw network. 
In this way, AlphaGo Zero utilizes an enhanced lookahead version of the raw network.

# Combination of the Policy and the Value Network into one
<hr style="border: 1px solid black;">

The most interesting part of combining the policy and the value network into one is to observe how the loss function changes. 
The loss function is a combination of the two well-known Mean Squared Error loss function and the Cross-Entropy loss function with L2-Regularization. 
The Mean Squared Error part is needed to train the value network, while the Cross-Entropy loss part is needed to train the policy network.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Loss_annotated.PNG" alt="Image of annotated loss function"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Performance of AlphaGo Zero
<hr style="border: 1px solid black;">

At day zero, AlphaGo Zero starts by playing random moves. 
After three days of training, AlphaGo Zero defeated AlphaGo Lee, the version that beat Lee Sedol, by a score of 100-0. 
After 27 days, AlphaGo Zero defeated AlphaGo Master, the version that beat Ke Jie, by a score of 89-11. 
After 40 days, AlphaGo Zero became the best Go player in the world, surpassing all previous versions of AlphaGo.

On the right one can see the elo-rating comparison of all AlphaGo versions in blue compared to each other, as well as the strength of the raw network in gray and previous algorithms in red. 
It is noteworhty, that choosing moves soley based on the  probability distribution outputted by the network is much weaker than the combination of the network with MCTS. 

The evaluation of the strength of the algorithms was done by playing games against previous versions of AlphaGo. 
The algorithms had five seconds of thinking time per move and 1600 simulations for each MCTS. 
The results are shown in the following figure.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaGoZero_Performance.png" alt="Image of AlphaGoZero_Performance"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# AlphaGo Zero discovers Opening Patterns
<hr style="border: 1px solid black;">

Over time humans came up with patterns of moves (especially in the beginning of a go match) that are considered to be the best. 
These patterns are called josekis. 
One of the most remarkable aspects of AlphaGo Zero is that it rediscovered human opening patterns that took thousands of years to develop, as well as some novel moves that were discarded or overlooked by humans.

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/AlphaZero_Pattern.png" alt="Image of AlphaZero Patterns"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Humans can learn from AlphaGo Zero
<hr style="border: 1px solid black;">

AlphaGo Zero not only rediscovered know josekis, but also invented new ones. 
This shows that the algorithm is able to learn from scratch and discover new strategies and patterns that were not known to human players. 
This is a remarkable demonstration of the power of self-learning and self-improvement in artificial intelligence.
Further, some josekis that were considered to be the best by humans were discarded by AlphaGo Zero, showing that the algorithm is able to challenge and improve human knowledge.
The following figure shows that AlphaGo Zero picks up some patterns but discardes them after more training if deemed inferior.

AlphaGo Zero also showed a remarkable level of understanding and intuition, by playing moves that were unconventional, but effective. 

<figure>
  <img src="https://tareqkomboz.github.io/forkedHauptseminar/images/Joseki_Frequency.png" alt="Image of Joseki Frequency"/>
  <figcaption>Source: Silver, David et al., 2017</figcaption>
</figure>

# Discussion
<hr style="border: 1px solid black;">

## Contributions

- **Superior Performance**: AlphaGo and its successor, AlphaGo Zero, have demonstrated superior performance of algorihtms over humans in the game of Go for the first time in history. Their victories against world-class human players have underscored the potential of artificial intelligence in mastering complex tasks.

- **Self-Learning**: One of the most remarkable aspects of AlphaGo Zero is its ability to learn from scratch, without any prior human knowledge. Through a process of self-play, it was able to acquire strategies and tactics that even the most skilled human players had not discovered.

- **Generalization**: The techniques used in AlphaGo are not limited to the game of Go. They represent a general approach to artificial intelligence that can be applied to a wide range of problems. Actually, the DeepMind team did generalize their approach of AlphaGo Zero to other board games, such as chess and shogi (Japanese chess). They developed a program called AlphaZero, which used the same algorithm and network architecture as AlphaGo Zero, but with different input and output layers to accommodate the different rules and board sizes. AlphaZero was able to learn chess and shogi from scratch, and defeat the world's best programs in both games, such as Stockfish (the 2016 world chess champion) and Elmo (the 2017 world shogi champion). For more details on AlphaZero, you can refer to the paper ["Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" by Silver, David et al. (2017)](https://arxiv.org/abs/1712.01815).

## Limitations

- **Lack of Explainability**: While the performance of AlphaGo and AlphaGo Zero is impressive, the neural networks they use are complex and difficult to interpret. This lack of explainability makes it challenging to understand the reasoning behind the moves made by the algorithms. Especially Beginners in Go might not find value in move suggestions as a move is only strong when understanding the follow up plan.

- **Data Efficiency**: AlphaGo Zero required a large amount of data—equivalent to more than a single human's lifetime experience—to achieve its performance. While this is still more efficient than previous methods, this suggests that there may still be room for improving the data efficiency of AI systems. On the other hand, it's worth noting that human players achieve high levels of skill by learning from the accumulated knowledge of previous generations through books and videos.
  
# Conclusion
<hr style="border: 1px solid black;">

I hope you enjoyed reading this blog post and learned something new.
In conclusion, the development and success of AlphaGo have marked a significant milestone in the field of artificial intelligence. 
AlphaGo Lee, by integrating existing MCTS techniques with Deep Learning, became the first computer program to triumph over a world champion in the complex game of Go. 
This achievement underscored the potential of combining traditional AI methods with modern machine learning techniques.
However, the evolution didn’t stop there. 
AlphaGo Zero, a subsequent version, demonstrated an even more profound capability. 
Trained entirely from scratch, without relying on any pre-existing human game data, it managed to outperform all other algorithms in Go.
AlphaGo and AlphaGo Zero have inspired human players to learn from their moves, and to improve their own skills and understanding.

# Acknowledgements
<hr style="border: 1px solid black;">

I would like to thank Prof. Dr. Mathias Niepert and M.Sc. Marimuthu Kalimuthu, for their guidance and support throughout this project. 

# References (Last accessed at 09.02.2024)
<hr style="border: 1px solid black;">

Papers
- [Silver, David, et al. "Mastering the game of Go with deep neural networks and tree search." nature 529.7587 (2016): 484-489.](https://www.rose-hulman.edu/class/cs/csse413/schedule/day16/MasteringTheGameofGo.pdf)
- [Silver, David, et al. "Mastering the game of go without human knowledge." nature 550.7676 (2017): 354-359.](https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf)
- [Silver, David, et al. "Mastering chess and shogi by self-play with a general reinforcement learning algorithm." arXiv preprint arXiv:1712.01815 (2017).](https://arxiv.org/pdf/1712.01815.pdf))
- [He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.](https://arxiv.org/pdf/1512.03385v1.pdf)

Blog Posts
- [AlphaGo](https://deepmind.google/technologies/alphago/)
- [AlphaGo Zero: Starting from scratch](https://deepmind.google/discover/blog/alphago-zero-starting-from-scratch/)
- [Innovations of AlphaGo](https://deepmind.google/discover/blog/innovations-of-alphago/)
- [Was ist ein Convolutional Neural Network?](https://de.mathworks.com/discovery/convolutional-neural-network-matlab.html)

Videos
- [AlphaGo - Mastering the game of Go with deep neural networks and tree search - RL Paper Explained](https://www.youtube.com/watch?v=Z1BELqFQZVM)
- [DeepMind's AlphaGo Zero and AlphaZero - RL paper explained](https://www.youtube.com/watch?v=0slFo1rV0EM)
- [Deepmind AlphaZero - Mastering Games Without Human Knowledge](https://www.youtube.com/watch?v=Wujy7OzvdJk)
